{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning with TF2 and Keras: chapter 08.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrFh6M3CNo3pW8uzdc6d7X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aboelela924/Deep-learning-with-TF2-and-Keras/blob/master/Deep_learning_with_TF2_and_Keras_chapter_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJWGLd8QkHVg"
      },
      "source": [
        "<h1>Text Generation</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egiVpc7tqVNc",
        "outputId": "eb98a2ff-5991-4fdb-b79e-58c1c98ffdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=dbea7ca6d74b8758558677ffc2b7c35f5a3024c381650bbf3a541bb8f823f887\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ2P35N0kHHS"
      },
      "source": [
        "import numpy as np \n",
        "import os \n",
        "import re \n",
        "import shutil \n",
        "import tensorflow as tf\n",
        "import wget\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf-ZP9nakDgo"
      },
      "source": [
        "DATA_DIR = \"/content/data\"\n",
        "CHECKPOINT_DIR = \"/content/data/checkpoints\"\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    os.mkdir(DATA_DIR)\n",
        "    os.mkdir(CHECKPOINT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4y3KuMopyMQ"
      },
      "source": [
        "def download_and_read(urls):\n",
        "    texts = []\n",
        "    for url in urls:\n",
        "        data = wget.download(url, DATA_DIR)\n",
        "        text = open(data, \"r\").read()\n",
        "        text = text.replace(\"\\ufeff\", \"\")\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        text = re.sub(\"\\s+\", \" \", text)\n",
        "        texts.extend(text)\n",
        "    return texts     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9v0iDWwqm53"
      },
      "source": [
        "texts = download_and_read([\n",
        "\"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
        "\"https://www.gutenberg.org/files/12/12-0.txt\"\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67UZueoPqpkQ",
        "outputId": "eaa5c866-4934-462a-a02f-41bef8933af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "vocab = sorted(set(texts))\n",
        "print(\"Vocabulary size: {:d}\".format(len(vocab)))\n",
        "\n",
        "char2idx = {c:i for i, c in enumerate(vocab)}\n",
        "idx2char = {i:c for c, i in char2idx.items()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MsNDR_QsQOK",
        "outputId": "02effad7-6948-4689-828c-424987a7c2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(char2idx)\n",
        "print(idx2char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '·': 84, 'ù': 85, '‘': 86, '’': 87, '“': 88, '”': 89}\n",
            "{0: ' ', 1: '!', 2: '\"', 3: '#', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: ',', 12: '-', 13: '.', 14: '/', 15: '0', 16: '1', 17: '2', 18: '3', 19: '4', 20: '5', 21: '6', 22: '7', 23: '8', 24: '9', 25: ':', 26: ';', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '·', 85: 'ù', 86: '‘', 87: '’', 88: '“', 89: '”'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqR3-pF-sin2"
      },
      "source": [
        "texts_as_int = [char2idx[c] for c in texts]\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices(texts_as_int)\n",
        "seq_length = 100\n",
        "sequences = data.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_train_labels(seq):\n",
        "    train = seq[0:-1]\n",
        "    labels = seq[1:]\n",
        "    return train, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlhwt-UjzCKS"
      },
      "source": [
        "sequences = sequences.map(split_train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZu3VGehzKq-"
      },
      "source": [
        "batch_size = 64\n",
        "steps_per_epoch = ( len(texts) // seq_length ) // batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs0VeAd6zouj"
      },
      "source": [
        "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9T9zbLXz2pI"
      },
      "source": [
        "class CharGenModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, num_timesteps, embedding_dim,\n",
        "                 rnn_output_dim, **kwargs):\n",
        "        \n",
        "        super(CharGenModel, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedd = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn_layer = tf.keras.layers.GRU(\n",
        "            embedding_dim, \n",
        "            stateful=True, \n",
        "            return_sequences=True)\n",
        "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedd(x)\n",
        "        x = self.rnn_layer(x)\n",
        "        x = self.dense_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTWcxqJ1340S"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "num_timesteps = 100\n",
        "embedding_dim = 256\n",
        "rnn_output_dim = 1024\n",
        "batch_size = 64\n",
        "\n",
        "character_generator = CharGenModel(vocab_size, num_timesteps, embedding_dim, rnn_output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIv80Q0G4dBy",
        "outputId": "d129e043-2492-4036-9253-4ca2e641b93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "character_generator.build(input_shape=(batch_size, seq_length))\n",
        "character_generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"char_gen_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  23040     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  394752    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  23130     \n",
            "=================================================================\n",
            "Total params: 440,922\n",
            "Trainable params: 440,922\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaijPt7s4nCa"
      },
      "source": [
        "def loss(labels, predictions):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions, from_logits=True)\n",
        "\n",
        "character_generator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
        "                            loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_VToyKJ5FBK"
      },
      "source": [
        "def generate_text(model, prefix_string, char2idx, \n",
        "                  idx2char, output_length, randomness_prop = 1):\n",
        "    prefix_int = [char2idx[c] for c in prefix_string]\n",
        "    prefix_int = tf.expand_dims(prefix_int, 0)\n",
        "    model.reset_states()\n",
        "    out = []\n",
        "    for i in range(output_length):\n",
        "        predictions = model(prefix_int)\n",
        "        predictions = tf.squeeze(predictions, 0) / randomness_prop\n",
        "        pred_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        prefix_int = tf.expand_dims([pred_id], 0)\n",
        "        out.append(idx2char[pred_id])\n",
        "    return prefix_string + \"\".join(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-RrXN95A4bh",
        "outputId": "ccc5899d-7297-432f-d87b-45c5be9e426f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 50\n",
        "\n",
        "for i in range(num_epochs//10):\n",
        "    checkpoint_path = \"/content/data/model_after_{:d}_epochs.ckpt\".format(i+1*10)\n",
        "    callbacks = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                    save_weights_only=True,\n",
        "                                                    verbose=1)\n",
        "    character_generator.fit_generator(dataset, steps_per_epoch=steps_per_epoch,\n",
        "                                      epochs=10)\n",
        "    \n",
        "    character_generator.save_weights(checkpoint_path)\n",
        "    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim, rnn_output_dim)\n",
        "    gen_model.load_weights(checkpoint_path).expect_partial()\n",
        "    gen_model.build(input_shape=(1, seq_length))\n",
        "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char, 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 1.0028\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9964\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9903\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9825\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9777\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9692\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9646\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9601\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9506\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9507\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedd.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedd.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Alice never do,’ said the Knight seem to a bug, down in a complying, but the only knew Liture the world wa\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9416\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9373\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9312\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9252\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9206\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9165\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9095\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9090\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9040\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8950\n",
            "Alice a child!’ she added, like her, very saw them not again. ‘Consider your hands or not.\" \"After watchin\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8938\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8895\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8855\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8803\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8777\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8704\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8705\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8636\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8624\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8626\n",
            "Alice only spoke as Alice had to do it?”--I she beg your pooleep, she couldn’t help saying, child,’ said t\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8584\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8557\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8511\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8486\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8455\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8426\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8383\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8350\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8337\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8292\n",
            "Alice with a rattle!’ said Alice. ‘It’s round her head trumble,\" said Alice, tait his off). And the Rabbit\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8253\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8256\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8255\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8199\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8195\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8184\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8139\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8120\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8096\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8089\n",
            "Alice begun to her places--all wondering hall, and when I grow a thing is the subject. \"Don't little good \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQjlLrihYR_v"
      },
      "source": [
        "<h1>Sentiment Analysis</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2CQLVmffFN4",
        "outputId": "f89f649d-a244-4f2d-b5a0-1dbdaa1e3212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=f184614ca17c9c5a12c4aed3cb41c13b9fc0b603f0f81c9fa1628f7408c4a211\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRkPWwDUR6Rg"
      },
      "source": [
        "import numpy as np \n",
        "import os \n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import wget \n",
        "import zipfile\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdaxnRsAfwcN"
      },
      "source": [
        "DATA_DIR = \"/content/data\"\n",
        "CHECKPOINT_DIR = \"/content/data/checkpoints\"\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    os.mkdir(DATA_DIR)\n",
        "    os.mkdir(CHECKPOINT_DIR)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoQ6z0YpSn3x"
      },
      "source": [
        "def download_and_read(url):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    data = wget.download(url, DATA_DIR)\n",
        "    with zipfile.ZipFile(data, \"r\") as ref:\n",
        "        ref.extractall(\"/content/data\")  \n",
        "    file_path = DATA_DIR + \"/\" + data.split(\"/\")[-1].split(\".\")[0]\n",
        "    if \"(\" in file_path:\n",
        "        file_path = file_path.split(\" (\")[0]\n",
        "    \n",
        "    for text_file in os.listdir(file_path):\n",
        "        if text_file.endswith(\"_labelled.txt\"):\n",
        "            with open(file_path+\"/\"+text_file, \"r\") as reader:\n",
        "                for line in reader:\n",
        "                    line = line.replace(\"\\n\", \"\")\n",
        "                    sentence, label = line.split(\"\\t\")\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(int(label))\n",
        "    return sentences, labels"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-lZLETcgDXZ"
      },
      "source": [
        "sentences, labels = download_and_read(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment labelled sentences.zip\")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v8zpXAmgKkU",
        "outputId": "572d6316-c156-4db0-feb6-89b0e26dd344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer  = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word2idx = tokenizer.word_index\n",
        "print(\"Vocab size: {:d}\".format(len(word2idx)))\n",
        "ids2word = {value: key for key, value in word2idx.items()}\n",
        "\n",
        "vocab_size = len(word2idx)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 5271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RpEWhuMl5gO",
        "outputId": "5b1a5974-578b-40b9-874c-dfeda6dd4bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences_length = np.array([ len(s.split()) for s in sentences])\n",
        "print([ (p, np.percentile(sentences_length, p)) for p in [75, 80, 85, 90, 95, 99, 100] ])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(75, 16.0), (80, 18.0), (85, 20.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWhHFje_p1r_"
      },
      "source": [
        "max_len = 64\n",
        "sentences_as_int = tokenizer.texts_to_sequences(sentences)\n",
        "sentences_as_int = tf.keras.preprocessing.sequence.pad_sequences(sentences_as_int, maxlen=max_len)\n",
        "labels_as_int = np.array(labels)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((sentences_as_int, labels_as_int))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEcSrOEErIxL"
      },
      "source": [
        "dataset = dataset.shuffle(10000)\n",
        "\n",
        "test_size = len(sentences) // 3\n",
        "val_size = ( len(sentences) - test_size ) // 10\n",
        "\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZdOxvvlslIL"
      },
      "source": [
        "class SentimentAnalysisModel(tf.keras.Model):\n",
        "    def __init__(self,vocab_size, max_len, **kwargs):\n",
        "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedd = tf.keras.layers.Embedding(vocab_size, max_len)\n",
        "        self.rnn = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(max_len)\n",
        "        )\n",
        "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedd(x)\n",
        "        x = self.rnn(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hWAEPf91hto",
        "outputId": "95302bb3-4445-4c3a-e55b-3728b1e05b13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SentimentAnalysisModel(vocab_size=vocab_size+1,max_len=max_len)\n",
        "model.build(input_shape=(batch_size, max_len))\n",
        "model.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sentiment_analysis_model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      multiple                  337408    \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection multiple                  66048     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             multiple                  8256      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             multiple                  65        \n",
            "=================================================================\n",
            "Total params: 411,777\n",
            "Trainable params: 411,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82yt7veC1-EY"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\", \n",
        "              optimizer=\"adam\", \n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J_0YyW42PXF",
        "outputId": "03133d30-ef7c-43b4-c513-e3219e49a629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_model_file = os.path.join(CHECKPOINT_DIR, \"best_model.h5\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file, \n",
        "                                                save_best_only=True,\n",
        "                                                save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "EPOCHS = 10\n",
        "history = model.fit(train_dataset, epochs=EPOCHS, \n",
        "                    validation_data=val_dataset, \n",
        "                    callbacks=[checkpoint])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25/29 [========================>.....] - ETA: 0s - loss: 0.0762 - accuracy: 0.9775\n",
            "Epoch 00001: val_loss improved from inf to 0.04226, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0745 - accuracy: 0.9778 - val_loss: 0.0423 - val_accuracy: 0.9800\n",
            "Epoch 2/10\n",
            "27/29 [==========================>...] - ETA: 0s - loss: 0.0340 - accuracy: 0.9913\n",
            "Epoch 00002: val_loss improved from 0.04226 to 0.02379, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0351 - accuracy: 0.9906 - val_loss: 0.0238 - val_accuracy: 0.9950\n",
            "Epoch 3/10\n",
            "27/29 [==========================>...] - ETA: 0s - loss: 0.0239 - accuracy: 0.9954\n",
            "Epoch 00003: val_loss improved from 0.02379 to 0.01395, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0239 - accuracy: 0.9956 - val_loss: 0.0140 - val_accuracy: 0.9950\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9972\n",
            "Epoch 00004: val_loss improved from 0.01395 to 0.00608, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0169 - accuracy: 0.9972 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "25/29 [========================>.....] - ETA: 0s - loss: 0.0145 - accuracy: 0.9975\n",
            "Epoch 00005: val_loss did not improve from 0.00608\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0136 - accuracy: 0.9978 - val_loss: 0.0590 - val_accuracy: 0.9900\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9978\n",
            "Epoch 00006: val_loss did not improve from 0.00608\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0159 - accuracy: 0.9978 - val_loss: 0.0327 - val_accuracy: 0.9950\n",
            "Epoch 7/10\n",
            "25/29 [========================>.....] - ETA: 0s - loss: 0.0119 - accuracy: 0.9975\n",
            "Epoch 00007: val_loss did not improve from 0.00608\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 0.9972 - val_loss: 0.0139 - val_accuracy: 0.9950\n",
            "Epoch 8/10\n",
            "28/29 [===========================>..] - ETA: 0s - loss: 0.0146 - accuracy: 0.9978\n",
            "Epoch 00008: val_loss improved from 0.00608 to 0.00545, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0146 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "28/29 [===========================>..] - ETA: 0s - loss: 0.0096 - accuracy: 0.9983\n",
            "Epoch 00009: val_loss improved from 0.00545 to 0.00172, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "25/29 [========================>.....] - ETA: 0s - loss: 0.0112 - accuracy: 0.9987\n",
            "Epoch 00010: val_loss did not improve from 0.00172\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0103 - accuracy: 0.9989 - val_loss: 0.0024 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F2l-vul3n-2"
      },
      "source": [
        "best_model = SentimentAnalysisModel(vocab_size+1, max_len)\n",
        "best_model.build(input_shape=(batch_size, max_len))\n",
        "best_model.load_weights(best_model_file)\n",
        "best_model.compile(\n",
        "loss=\"binary_crossentropy\",\n",
        "optimizer=\"adam\",\n",
        "metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ybydoh56uej",
        "outputId": "2e9f9d38-877f-4456-aaa2-dd58f3eb4301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
        "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9990\n",
            "test loss: 0.012, test accuracy: 0.999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}