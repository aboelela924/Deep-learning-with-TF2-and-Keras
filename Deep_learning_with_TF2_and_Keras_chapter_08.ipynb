{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning with TF2 and Keras: chapter 08.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9BKzuUyQsaa/rEu4p71YK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aboelela924/Deep-learning-with-TF2-and-Keras/blob/master/Deep_learning_with_TF2_and_Keras_chapter_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJWGLd8QkHVg"
      },
      "source": [
        "<h1>Text Generation</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egiVpc7tqVNc",
        "outputId": "eb98a2ff-5991-4fdb-b79e-58c1c98ffdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=dbea7ca6d74b8758558677ffc2b7c35f5a3024c381650bbf3a541bb8f823f887\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ2P35N0kHHS"
      },
      "source": [
        "import numpy as np \n",
        "import os \n",
        "import re \n",
        "import shutil \n",
        "import tensorflow as tf\n",
        "import wget\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf-ZP9nakDgo"
      },
      "source": [
        "DATA_DIR = \"/content/data\"\n",
        "CHECKPOINT_DIR = \"/content/data/checkpoints\"\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    os.mkdir(DATA_DIR)\n",
        "    os.mkdir(CHECKPOINT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4y3KuMopyMQ"
      },
      "source": [
        "def download_and_read(urls):\n",
        "    texts = []\n",
        "    for url in urls:\n",
        "        data = wget.download(url, DATA_DIR)\n",
        "        text = open(data, \"r\").read()\n",
        "        text = text.replace(\"\\ufeff\", \"\")\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        text = re.sub(\"\\s+\", \" \", text)\n",
        "        texts.extend(text)\n",
        "    return texts     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9v0iDWwqm53"
      },
      "source": [
        "texts = download_and_read([\n",
        "\"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
        "\"https://www.gutenberg.org/files/12/12-0.txt\"\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67UZueoPqpkQ",
        "outputId": "eaa5c866-4934-462a-a02f-41bef8933af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "vocab = sorted(set(texts))\n",
        "print(\"Vocabulary size: {:d}\".format(len(vocab)))\n",
        "\n",
        "char2idx = {c:i for i, c in enumerate(vocab)}\n",
        "idx2char = {i:c for c, i in char2idx.items()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MsNDR_QsQOK",
        "outputId": "02effad7-6948-4689-828c-424987a7c2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(char2idx)\n",
        "print(idx2char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '?': 27, '@': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '·': 84, 'ù': 85, '‘': 86, '’': 87, '“': 88, '”': 89}\n",
            "{0: ' ', 1: '!', 2: '\"', 3: '#', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: ',', 12: '-', 13: '.', 14: '/', 15: '0', 16: '1', 17: '2', 18: '3', 19: '4', 20: '5', 21: '6', 22: '7', 23: '8', 24: '9', 25: ':', 26: ';', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '·', 85: 'ù', 86: '‘', 87: '’', 88: '“', 89: '”'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqR3-pF-sin2"
      },
      "source": [
        "texts_as_int = [char2idx[c] for c in texts]\n",
        "\n",
        "data = tf.data.Dataset.from_tensor_slices(texts_as_int)\n",
        "seq_length = 100\n",
        "sequences = data.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_train_labels(seq):\n",
        "    train = seq[0:-1]\n",
        "    labels = seq[1:]\n",
        "    return train, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlhwt-UjzCKS"
      },
      "source": [
        "sequences = sequences.map(split_train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZu3VGehzKq-"
      },
      "source": [
        "batch_size = 64\n",
        "steps_per_epoch = ( len(texts) // seq_length ) // batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs0VeAd6zouj"
      },
      "source": [
        "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9T9zbLXz2pI"
      },
      "source": [
        "class CharGenModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, num_timesteps, embedding_dim,\n",
        "                 rnn_output_dim, **kwargs):\n",
        "        \n",
        "        super(CharGenModel, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedd = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn_layer = tf.keras.layers.GRU(\n",
        "            embedding_dim, \n",
        "            stateful=True, \n",
        "            return_sequences=True)\n",
        "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedd(x)\n",
        "        x = self.rnn_layer(x)\n",
        "        x = self.dense_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTWcxqJ1340S"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "num_timesteps = 100\n",
        "embedding_dim = 256\n",
        "rnn_output_dim = 1024\n",
        "batch_size = 64\n",
        "\n",
        "character_generator = CharGenModel(vocab_size, num_timesteps, embedding_dim, rnn_output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIv80Q0G4dBy",
        "outputId": "d129e043-2492-4036-9253-4ca2e641b93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "character_generator.build(input_shape=(batch_size, seq_length))\n",
        "character_generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"char_gen_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  23040     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  394752    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  23130     \n",
            "=================================================================\n",
            "Total params: 440,922\n",
            "Trainable params: 440,922\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaijPt7s4nCa"
      },
      "source": [
        "def loss(labels, predictions):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions, from_logits=True)\n",
        "\n",
        "character_generator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
        "                            loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_VToyKJ5FBK"
      },
      "source": [
        "def generate_text(model, prefix_string, char2idx, \n",
        "                  idx2char, output_length, randomness_prop = 1):\n",
        "    prefix_int = [char2idx[c] for c in prefix_string]\n",
        "    prefix_int = tf.expand_dims(prefix_int, 0)\n",
        "    model.reset_states()\n",
        "    out = []\n",
        "    for i in range(output_length):\n",
        "        predictions = model(prefix_int)\n",
        "        predictions = tf.squeeze(predictions, 0) / randomness_prop\n",
        "        pred_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        prefix_int = tf.expand_dims([pred_id], 0)\n",
        "        out.append(idx2char[pred_id])\n",
        "    return prefix_string + \"\".join(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-RrXN95A4bh",
        "outputId": "ccc5899d-7297-432f-d87b-45c5be9e426f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 50\n",
        "\n",
        "for i in range(num_epochs//10):\n",
        "    checkpoint_path = \"/content/data/model_after_{:d}_epochs.ckpt\".format(i+1*10)\n",
        "    callbacks = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                    save_weights_only=True,\n",
        "                                                    verbose=1)\n",
        "    character_generator.fit_generator(dataset, steps_per_epoch=steps_per_epoch,\n",
        "                                      epochs=10)\n",
        "    \n",
        "    character_generator.save_weights(checkpoint_path)\n",
        "    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim, rnn_output_dim)\n",
        "    gen_model.load_weights(checkpoint_path).expect_partial()\n",
        "    gen_model.build(input_shape=(1, seq_length))\n",
        "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char, 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 1.0028\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9964\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9903\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9825\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9777\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9692\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9646\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9601\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9506\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9507\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedd.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedd.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Alice never do,’ said the Knight seem to a bug, down in a complying, but the only knew Liture the world wa\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9416\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9373\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9312\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9252\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9206\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.9165\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9095\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9090\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.9040\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8950\n",
            "Alice a child!’ she added, like her, very saw them not again. ‘Consider your hands or not.\" \"After watchin\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8938\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8895\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8855\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8803\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8777\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8704\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8705\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8636\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8624\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8626\n",
            "Alice only spoke as Alice had to do it?”--I she beg your pooleep, she couldn’t help saying, child,’ said t\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8584\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8557\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8511\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8486\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8455\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8426\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8383\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8350\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8337\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8292\n",
            "Alice with a rattle!’ said Alice. ‘It’s round her head trumble,\" said Alice, tait his off). And the Rabbit\n",
            "Epoch 1/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8253\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8256\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.8255\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8199\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8195\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8184\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8139\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8120\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8096\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.8089\n",
            "Alice begun to her places--all wondering hall, and when I grow a thing is the subject. \"Don't little good \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQjlLrihYR_v"
      },
      "source": [
        "<h1>Sentiment Analysis</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2CQLVmffFN4",
        "outputId": "f89f649d-a244-4f2d-b5a0-1dbdaa1e3212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=f184614ca17c9c5a12c4aed3cb41c13b9fc0b603f0f81c9fa1628f7408c4a211\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRkPWwDUR6Rg"
      },
      "source": [
        "import numpy as np \n",
        "import os \n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import wget \n",
        "import zipfile\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdaxnRsAfwcN"
      },
      "source": [
        "DATA_DIR = \"/content/data\"\n",
        "CHECKPOINT_DIR = \"/content/data/checkpoints\"\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    os.mkdir(DATA_DIR)\n",
        "    os.mkdir(CHECKPOINT_DIR)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoQ6z0YpSn3x"
      },
      "source": [
        "def download_and_read(url):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    data = wget.download(url, DATA_DIR)\n",
        "    with zipfile.ZipFile(data, \"r\") as ref:\n",
        "        ref.extractall(\"/content/data\")  \n",
        "    file_path = DATA_DIR + \"/\" + data.split(\"/\")[-1].split(\".\")[0]\n",
        "    if \"(\" in file_path:\n",
        "        file_path = file_path.split(\" (\")[0]\n",
        "    \n",
        "    for text_file in os.listdir(file_path):\n",
        "        if text_file.endswith(\"_labelled.txt\"):\n",
        "            with open(file_path+\"/\"+text_file, \"r\") as reader:\n",
        "                for line in reader:\n",
        "                    line = line.replace(\"\\n\", \"\")\n",
        "                    sentence, label = line.split(\"\\t\")\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(int(label))\n",
        "    return sentences, labels"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-lZLETcgDXZ"
      },
      "source": [
        "sentences, labels = download_and_read(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment labelled sentences.zip\")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v8zpXAmgKkU",
        "outputId": "572d6316-c156-4db0-feb6-89b0e26dd344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer  = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word2idx = tokenizer.word_index\n",
        "print(\"Vocab size: {:d}\".format(len(word2idx)))\n",
        "ids2word = {value: key for key, value in word2idx.items()}\n",
        "\n",
        "vocab_size = len(word2idx)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 5271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RpEWhuMl5gO",
        "outputId": "5b1a5974-578b-40b9-874c-dfeda6dd4bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences_length = np.array([ len(s.split()) for s in sentences])\n",
        "print([ (p, np.percentile(sentences_length, p)) for p in [75, 80, 85, 90, 95, 99, 100] ])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(75, 16.0), (80, 18.0), (85, 20.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWhHFje_p1r_"
      },
      "source": [
        "max_len = 64\n",
        "sentences_as_int = tokenizer.texts_to_sequences(sentences)\n",
        "sentences_as_int = tf.keras.preprocessing.sequence.pad_sequences(sentences_as_int, maxlen=max_len)\n",
        "labels_as_int = np.array(labels)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((sentences_as_int, labels_as_int))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEcSrOEErIxL"
      },
      "source": [
        "dataset = dataset.shuffle(10000)\n",
        "\n",
        "test_size = len(sentences) // 3\n",
        "val_size = ( len(sentences) - test_size ) // 10\n",
        "\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZdOxvvlslIL"
      },
      "source": [
        "class SentimentAnalysisModel(tf.keras.Model):\n",
        "    def __init__(self,vocab_size, max_len, **kwargs):\n",
        "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedd = tf.keras.layers.Embedding(vocab_size, max_len)\n",
        "        self.rnn = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(max_len)\n",
        "        )\n",
        "        self.fc1 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedd(x)\n",
        "        x = self.rnn(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hWAEPf91hto",
        "outputId": "95302bb3-4445-4c3a-e55b-3728b1e05b13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SentimentAnalysisModel(vocab_size=vocab_size+1,max_len=max_len)\n",
        "model.build(input_shape=(batch_size, max_len))\n",
        "model.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sentiment_analysis_model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      multiple                  337408    \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection multiple                  66048     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             multiple                  8256      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             multiple                  65        \n",
            "=================================================================\n",
            "Total params: 411,777\n",
            "Trainable params: 411,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82yt7veC1-EY"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\", \n",
        "              optimizer=\"adam\", \n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J_0YyW42PXF",
        "outputId": "03133d30-ef7c-43b4-c513-e3219e49a629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_model_file = os.path.join(CHECKPOINT_DIR, \"best_model.h5\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file, \n",
        "                                                save_best_only=True,\n",
        "                                                save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "EPOCHS = 10\n",
        "history = model.fit(train_dataset, epochs=EPOCHS, \n",
        "                    validation_data=val_dataset, \n",
        "                    callbacks=[checkpoint])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25/29 [========================>.....] - ETA: 0s - loss: 0.0762 - accuracy: 0.9775\n",
            "Epoch 00001: val_loss improved from inf to 0.04226, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0745 - accuracy: 0.9778 - val_loss: 0.0423 - val_accuracy: 0.9800\n",
            "Epoch 2/10\n",
            "27/29 [==========================>...] - ETA: 0s - loss: 0.0340 - accuracy: 0.9913\n",
            "Epoch 00002: val_loss improved from 0.04226 to 0.02379, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0351 - accuracy: 0.9906 - val_loss: 0.0238 - val_accuracy: 0.9950\n",
            "Epoch 3/10\n",
            "27/29 [==========================>...] - ETA: 0s - loss: 0.0239 - accuracy: 0.9954\n",
            "Epoch 00003: val_loss improved from 0.02379 to 0.01395, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0239 - accuracy: 0.9956 - val_loss: 0.0140 - val_accuracy: 0.9950\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9972\n",
            "Epoch 00004: val_loss improved from 0.01395 to 0.00608, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0169 - accuracy: 0.9972 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "25/29 [========================>.....] - ETA: 0s - loss: 0.0145 - accuracy: 0.9975\n",
            "Epoch 00005: val_loss did not improve from 0.00608\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0136 - accuracy: 0.9978 - val_loss: 0.0590 - val_accuracy: 0.9900\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9978\n",
            "Epoch 00006: val_loss did not improve from 0.00608\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0159 - accuracy: 0.9978 - val_loss: 0.0327 - val_accuracy: 0.9950\n",
            "Epoch 7/10\n",
            "25/29 [========================>.....] - ETA: 0s - loss: 0.0119 - accuracy: 0.9975\n",
            "Epoch 00007: val_loss did not improve from 0.00608\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 0.9972 - val_loss: 0.0139 - val_accuracy: 0.9950\n",
            "Epoch 8/10\n",
            "28/29 [===========================>..] - ETA: 0s - loss: 0.0146 - accuracy: 0.9978\n",
            "Epoch 00008: val_loss improved from 0.00608 to 0.00545, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0146 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "28/29 [===========================>..] - ETA: 0s - loss: 0.0096 - accuracy: 0.9983\n",
            "Epoch 00009: val_loss improved from 0.00545 to 0.00172, saving model to /content/data/checkpoints/best_model.h5\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "25/29 [========================>.....] - ETA: 0s - loss: 0.0112 - accuracy: 0.9987\n",
            "Epoch 00010: val_loss did not improve from 0.00172\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0103 - accuracy: 0.9989 - val_loss: 0.0024 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F2l-vul3n-2"
      },
      "source": [
        "best_model = SentimentAnalysisModel(vocab_size+1, max_len)\n",
        "best_model.build(input_shape=(batch_size, max_len))\n",
        "best_model.load_weights(best_model_file)\n",
        "best_model.compile(\n",
        "loss=\"binary_crossentropy\",\n",
        "optimizer=\"adam\",\n",
        "metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ybydoh56uej",
        "outputId": "2e9f9d38-877f-4456-aaa2-dd58f3eb4301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
        "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9990\n",
            "test loss: 0.012, test accuracy: 0.999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIsr8uVp7GoF"
      },
      "source": [
        "<h1>POS tagging</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMzLB6Y97Iw3"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh8x4t2u_Ru-",
        "outputId": "79d7ab21-67c9-46dd-c9fd-3df528454900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download(\"treebank\")"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JbmC6-o_WtH"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import shutil\n",
        "import os"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq-EZMKFLG0n"
      },
      "source": [
        "DATA_DIR = \"/content/data\"\n",
        "CHECKPOINT_DIR = \"/content/data/checkpoints\"\n",
        "DATASETS_DIR = \"/content/data/datasets\"\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    os.mkdir(DATA_DIR)\n",
        "    os.mkdir(CHECKPOINT_DIR)\n",
        "    os.mkdir(DATASETS_DIR)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo04ra94Kvwy"
      },
      "source": [
        "def download_and_read(datasetdir):\n",
        "    sent_filename = os.path.join(datasetdir, \"treebank-sents.txt\")\n",
        "    poss_filename = os.path.join(datasetdir, \"treebank-poss.txt\")\n",
        "\n",
        "    fsents = open(sent_filename, \"w\")\n",
        "    fposs = open(poss_filename, \"w\")\n",
        "\n",
        "    sentences = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        fsents.write(\" \".join([ w for w, p in sentence ]) +\"\\n\")\n",
        "        fposs.write(\" \".join([ p for w, p in sentence ]) + \"\\n\")\n",
        "\n",
        "    fsents.close()\n",
        "    fposs.close()\n",
        "\n",
        "    sents = []\n",
        "    posses = []\n",
        "\n",
        "    with open(sent_filename, \"r\") as fsents:\n",
        "        for idx, line in enumerate(fsents):\n",
        "            sents.append(line.strip())\n",
        "    with open(poss_filename, \"r\") as fposs:\n",
        "        for idx, line in enumerate(fposs):\n",
        "            posses.append(line.strip())  \n",
        "\n",
        "    return sents, posses  "
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXNTKrPkMbSg"
      },
      "source": [
        "def tokenize_and_buid_vocab(texts, vocab_size=None, lower=True):\n",
        "    if vocab_size is None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
        "    else:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size+1, lower=lower, oov_token=\"UNK\")\n",
        "    \n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    if vocab_size is not None:\n",
        "        tokenizer.word_index = {word:index for word, index in tokenizer.word_index.items() if index <= vocab_size+1}\n",
        "\n",
        "    word2idx = tokenizer.word_index\n",
        "    idx2word = {index: word for word, index in word2idx.items()}\n",
        "\n",
        "    return word2idx, idx2word, tokenizer"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rNf9uaaMZPH",
        "outputId": "bcf63331-225d-4b34-aae3-8e8bdf9bbcf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences, poss = download_and_read(DATASETS_DIR)\n",
        "assert(len(sentences) == len(poss))\n",
        "print(\"# of records: {:d}\".format(len(sentences)))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of records: 3914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjGlpoGjaTSo"
      },
      "source": [
        "sentences_vocab_size = 9000\n",
        "poss_vocab_size = 38\n",
        "\n",
        "word2idx_sents, idx2word_sents, tokenizer_sents = tokenize_and_buid_vocab(sentences, vocab_size=sentences_vocab_size)\n",
        "word2idx_poss, idx2word_poss, tokenizer_poss = tokenize_and_buid_vocab(poss, vocab_size=poss_vocab_size, lower=False)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyQifzc5qBdj"
      },
      "source": [
        "idx2word_sents[0], idx2word_poss[0] = \"PAD\", \"PAD\""
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Zddw4GWakTv",
        "outputId": "af0df155-f7d6-477e-f881-219b43abd56d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "source_vocab_size = len(word2idx_sents)\n",
        "target_vocab_size = len(word2idx_poss)\n",
        "print(\"vocab sizes (source): {:d}, (target): {:d}\".format(source_vocab_size, target_vocab_size))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab sizes (source): 9001, (target): 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iGphL0cavE5",
        "outputId": "2627a498-177d-4434-da56-912375027275",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sequences_length = np.array([len(s.split()) for s in sentences])\n",
        "print([(p, np.percentile(sequences_length, p)) for p in [75, 80, 85, 90, 95, 99, 100]])"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(75, 33.0), (80, 35.0), (85, 38.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aT2CDdtbyk-"
      },
      "source": [
        "max_len = 271\n",
        "\n",
        "sents_to_ints = tokenizer_sents.texts_to_sequences(sentences)\n",
        "sents_to_ints = tf.keras.preprocessing.sequence.pad_sequences(sents_to_ints, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "poss_to_ints = tokenizer_poss.texts_to_sequences(poss)\n",
        "poss_to_ints = tf.keras.preprocessing.sequence.pad_sequences(poss_to_ints, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "poss_to_cat_ints = []\n",
        "for p in poss_to_ints:\n",
        "    poss_to_cat_ints.append(tf.keras.utils.to_categorical(p, num_classes=poss_vocab_size+1, dtype=\"int32\"))\n",
        "poss_to_cat_ints = tf.keras.preprocessing.sequence.pad_sequences(poss_to_cat_ints, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((sents_to_ints, poss_to_cat_ints))"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uROErAFDdSN7"
      },
      "source": [
        "dataset = dataset.shuffle(10000)\n",
        "\n",
        "test_dataset_length = len(sentences) // 3\n",
        "val_dataset_length = (len(sentences) - test_dataset_length) // 10\n",
        "\n",
        "test_dataset = dataset.take(test_dataset_length)\n",
        "val_dataset = dataset.skip(test_dataset_length).take(val_dataset_length)\n",
        "train_dataset = dataset.skip(test_dataset_length+val_dataset_length)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqCcBUJ7fm2S"
      },
      "source": [
        "batch_size = 128\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JhcqJfvi3pC"
      },
      "source": [
        "def masked_accuracy():\n",
        "    def masked_accuracy_fn(ytrue, ypred):\n",
        "        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
        "        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
        " \n",
        "        mask = tf.keras.backend.cast(\n",
        "            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
        "        matches = tf.keras.backend.cast(\n",
        "            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
        "        numer = tf.keras.backend.sum(matches)\n",
        "        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n",
        "        accuracy =  numer / denom\n",
        "        return accuracy\n",
        "\n",
        "    return masked_accuracy_fn"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovwKhbGOkbDv"
      },
      "source": [
        "class POSTaggingModel(tf.keras.Model):\n",
        "    def __init__(self, source_vocab_size, traget_vocab_size, \n",
        "                 max_len, embedding_dim, rnn_out_dim, **kwargs):\n",
        "        super(POSTaggingModel, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(source_vocab_size, \n",
        "                                                   embedding_dim, \n",
        "                                                   input_length=max_len)\n",
        "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
        "        self.rnn = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.GRU(rnn_out_dim, return_sequences=True)\n",
        "        )\n",
        "        self.fc1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(target_vocab_size))\n",
        "        self.activation = tf.keras.layers.Activation(\"softmax\")\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.rnn(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQDQjkOMyqnl"
      },
      "source": [
        "embedding_dim = 128\n",
        "rnn_output_dim = 256"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnjO76FUzEKX"
      },
      "source": [
        "model = POSTaggingModel(source_vocab_size, target_vocab_size, max_len, \n",
        "                        embedding_dim, rnn_output_dim)\n",
        "model.build(input_shape=(batch_size, max_len))"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp6yMilrzhvJ",
        "outputId": "c5f052da-4e1e-4267-f22e-96267aea8984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"pos_tagging_model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     multiple                  1152128   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_12 (Spatia multiple                  0         \n",
            "_________________________________________________________________\n",
            "bidirectional_12 (Bidirectio multiple                  592896    \n",
            "_________________________________________________________________\n",
            "time_distributed_12 (TimeDis multiple                  20007     \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   multiple                  0         \n",
            "=================================================================\n",
            "Total params: 1,765,031\n",
            "Trainable params: 1,765,031\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIVM5kknzmOY"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=\"adam\", \n",
        "              metrics=[\"accuracy\", masked_accuracy()])"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJhgLd1Vz3O-",
        "outputId": "3d19c19f-5862-4fe5-b04c-9ba2cde42c00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "\n",
        "best_model_file = os.path.join(CHECKPOINT_DIR, \"best_model.h5\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                    best_model_file,\n",
        "                                    save_weights_only=True,\n",
        "                                    save_best_only=True)\n",
        "history = model.fit(train_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=val_dataset)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "19/19 [==============================] - 2s 127ms/step - loss: 1.4451 - accuracy: 0.8668 - masked_accuracy_fn: 0.0026 - val_loss: 0.3257 - val_accuracy: 0.9128 - val_masked_accuracy_fn: 0.0000e+00\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.3231 - accuracy: 0.9184 - masked_accuracy_fn: 0.0803 - val_loss: 0.3231 - val_accuracy: 0.9220 - val_masked_accuracy_fn: 0.1253\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.3147 - accuracy: 0.9225 - masked_accuracy_fn: 0.1906 - val_loss: 0.3184 - val_accuracy: 0.9148 - val_masked_accuracy_fn: 0.1889\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.3015 - accuracy: 0.9165 - masked_accuracy_fn: 0.1234 - val_loss: 0.2867 - val_accuracy: 0.9178 - val_masked_accuracy_fn: 0.1301\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.2814 - accuracy: 0.9176 - masked_accuracy_fn: 0.0917 - val_loss: 0.2740 - val_accuracy: 0.9218 - val_masked_accuracy_fn: 0.1379\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.2566 - accuracy: 0.9275 - masked_accuracy_fn: 0.1576 - val_loss: 0.2514 - val_accuracy: 0.9284 - val_masked_accuracy_fn: 0.1950\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.2535 - accuracy: 0.9252 - masked_accuracy_fn: 0.1431 - val_loss: 0.2418 - val_accuracy: 0.9273 - val_masked_accuracy_fn: 0.1208\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.2448 - accuracy: 0.9276 - masked_accuracy_fn: 0.1558 - val_loss: 0.2418 - val_accuracy: 0.9286 - val_masked_accuracy_fn: 0.1675\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.2408 - accuracy: 0.9302 - masked_accuracy_fn: 0.1933 - val_loss: 0.2293 - val_accuracy: 0.9352 - val_masked_accuracy_fn: 0.2359\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.2350 - accuracy: 0.9353 - masked_accuracy_fn: 0.2574 - val_loss: 0.2391 - val_accuracy: 0.9346 - val_masked_accuracy_fn: 0.3028\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.2244 - accuracy: 0.9392 - masked_accuracy_fn: 0.2983 - val_loss: 0.2291 - val_accuracy: 0.9380 - val_masked_accuracy_fn: 0.3086\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.2177 - accuracy: 0.9413 - masked_accuracy_fn: 0.3368 - val_loss: 0.2019 - val_accuracy: 0.9453 - val_masked_accuracy_fn: 0.3765\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.2042 - accuracy: 0.9444 - masked_accuracy_fn: 0.3665 - val_loss: 0.1888 - val_accuracy: 0.9475 - val_masked_accuracy_fn: 0.3249\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1924 - accuracy: 0.9477 - masked_accuracy_fn: 0.4049 - val_loss: 0.1852 - val_accuracy: 0.9503 - val_masked_accuracy_fn: 0.4141\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1850 - accuracy: 0.9501 - masked_accuracy_fn: 0.4326 - val_loss: 0.1816 - val_accuracy: 0.9513 - val_masked_accuracy_fn: 0.4313\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1747 - accuracy: 0.9536 - masked_accuracy_fn: 0.4719 - val_loss: 0.1777 - val_accuracy: 0.9528 - val_masked_accuracy_fn: 0.4798\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1705 - accuracy: 0.9550 - masked_accuracy_fn: 0.4924 - val_loss: 0.1571 - val_accuracy: 0.9577 - val_masked_accuracy_fn: 0.5411\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1572 - accuracy: 0.9586 - masked_accuracy_fn: 0.5190 - val_loss: 0.1535 - val_accuracy: 0.9599 - val_masked_accuracy_fn: 0.5199\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1561 - accuracy: 0.9590 - masked_accuracy_fn: 0.5364 - val_loss: 0.1499 - val_accuracy: 0.9604 - val_masked_accuracy_fn: 0.6273\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1469 - accuracy: 0.9613 - masked_accuracy_fn: 0.5600 - val_loss: 0.1399 - val_accuracy: 0.9628 - val_masked_accuracy_fn: 0.5771\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1435 - accuracy: 0.9619 - masked_accuracy_fn: 0.5681 - val_loss: 0.1442 - val_accuracy: 0.9619 - val_masked_accuracy_fn: 0.6078\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1364 - accuracy: 0.9637 - masked_accuracy_fn: 0.5823 - val_loss: 0.1290 - val_accuracy: 0.9653 - val_masked_accuracy_fn: 0.6146\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1344 - accuracy: 0.9637 - masked_accuracy_fn: 0.5921 - val_loss: 0.1179 - val_accuracy: 0.9678 - val_masked_accuracy_fn: 0.6486\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1284 - accuracy: 0.9647 - masked_accuracy_fn: 0.5995 - val_loss: 0.1152 - val_accuracy: 0.9689 - val_masked_accuracy_fn: 0.6665\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1230 - accuracy: 0.9657 - masked_accuracy_fn: 0.6085 - val_loss: 0.1318 - val_accuracy: 0.9637 - val_masked_accuracy_fn: 0.5578\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1193 - accuracy: 0.9666 - masked_accuracy_fn: 0.6183 - val_loss: 0.1126 - val_accuracy: 0.9685 - val_masked_accuracy_fn: 0.6609\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1174 - accuracy: 0.9668 - masked_accuracy_fn: 0.6276 - val_loss: 0.1098 - val_accuracy: 0.9683 - val_masked_accuracy_fn: 0.6042\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1145 - accuracy: 0.9674 - masked_accuracy_fn: 0.6305 - val_loss: 0.1118 - val_accuracy: 0.9667 - val_masked_accuracy_fn: 0.6013\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1112 - accuracy: 0.9680 - masked_accuracy_fn: 0.6408 - val_loss: 0.0979 - val_accuracy: 0.9712 - val_masked_accuracy_fn: 0.6186\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1064 - accuracy: 0.9691 - masked_accuracy_fn: 0.6524 - val_loss: 0.1021 - val_accuracy: 0.9701 - val_masked_accuracy_fn: 0.7158\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.1041 - accuracy: 0.9699 - masked_accuracy_fn: 0.6604 - val_loss: 0.1074 - val_accuracy: 0.9679 - val_masked_accuracy_fn: 0.6410\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.1001 - accuracy: 0.9708 - masked_accuracy_fn: 0.6697 - val_loss: 0.0896 - val_accuracy: 0.9735 - val_masked_accuracy_fn: 0.7079\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.0982 - accuracy: 0.9713 - masked_accuracy_fn: 0.6715 - val_loss: 0.0970 - val_accuracy: 0.9718 - val_masked_accuracy_fn: 0.6571\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.0978 - accuracy: 0.9713 - masked_accuracy_fn: 0.6808 - val_loss: 0.1039 - val_accuracy: 0.9693 - val_masked_accuracy_fn: 0.6328\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.0923 - accuracy: 0.9730 - masked_accuracy_fn: 0.6920 - val_loss: 0.0858 - val_accuracy: 0.9748 - val_masked_accuracy_fn: 0.7378\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.0923 - accuracy: 0.9729 - masked_accuracy_fn: 0.6919 - val_loss: 0.0885 - val_accuracy: 0.9737 - val_masked_accuracy_fn: 0.7261\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.0895 - accuracy: 0.9738 - masked_accuracy_fn: 0.7022 - val_loss: 0.0782 - val_accuracy: 0.9765 - val_masked_accuracy_fn: 0.6902\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.0856 - accuracy: 0.9750 - masked_accuracy_fn: 0.7162 - val_loss: 0.0840 - val_accuracy: 0.9749 - val_masked_accuracy_fn: 0.7449\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.0869 - accuracy: 0.9747 - masked_accuracy_fn: 0.7141 - val_loss: 0.0752 - val_accuracy: 0.9777 - val_masked_accuracy_fn: 0.7143\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 0.0848 - accuracy: 0.9749 - masked_accuracy_fn: 0.7159 - val_loss: 0.0805 - val_accuracy: 0.9764 - val_masked_accuracy_fn: 0.6946\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.0806 - accuracy: 0.9761 - masked_accuracy_fn: 0.7268 - val_loss: 0.0780 - val_accuracy: 0.9771 - val_masked_accuracy_fn: 0.6966\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.0807 - accuracy: 0.9762 - masked_accuracy_fn: 0.7273 - val_loss: 0.0758 - val_accuracy: 0.9775 - val_masked_accuracy_fn: 0.7835\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.0794 - accuracy: 0.9763 - masked_accuracy_fn: 0.7307 - val_loss: 0.0684 - val_accuracy: 0.9795 - val_masked_accuracy_fn: 0.8422\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.0787 - accuracy: 0.9764 - masked_accuracy_fn: 0.7355 - val_loss: 0.0756 - val_accuracy: 0.9775 - val_masked_accuracy_fn: 0.7566\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 2s 95ms/step - loss: 0.0748 - accuracy: 0.9777 - masked_accuracy_fn: 0.7427 - val_loss: 0.0740 - val_accuracy: 0.9786 - val_masked_accuracy_fn: 0.7674\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.0742 - accuracy: 0.9778 - masked_accuracy_fn: 0.7485 - val_loss: 0.0666 - val_accuracy: 0.9791 - val_masked_accuracy_fn: 0.7346\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 2s 98ms/step - loss: 0.0718 - accuracy: 0.9782 - masked_accuracy_fn: 0.7542 - val_loss: 0.0709 - val_accuracy: 0.9787 - val_masked_accuracy_fn: 0.7354\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.0720 - accuracy: 0.9784 - masked_accuracy_fn: 0.7577 - val_loss: 0.0660 - val_accuracy: 0.9799 - val_masked_accuracy_fn: 0.7774\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.0680 - accuracy: 0.9797 - masked_accuracy_fn: 0.7685 - val_loss: 0.0649 - val_accuracy: 0.9810 - val_masked_accuracy_fn: 0.7422\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 0.0668 - accuracy: 0.9800 - masked_accuracy_fn: 0.7767 - val_loss: 0.0654 - val_accuracy: 0.9804 - val_masked_accuracy_fn: 0.7944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vse0k7cc0CUY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}